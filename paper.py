#
# Code for the confidence intervals of variance components REML estimators
#
# Author: Regev Schweiger, Tel Aviv University, 2015
#  

from numpy import *

import numpy.linalg, numpy.random
import scipy.stats, scipy.sparse
import socket, os.path, cPickle, csv, bisect, time, tempfile, subprocess


# pylmm is available from: https://github.com/nickFurlotte/pylmm
try:
    import pylmm.input, pylmm.lmm
    import pylmm.lmm_unbounded
except:
    pass


################################################################################################################
#
# General 
#


# Use progress bar is available
itrange = locals().get('trange', range)

################################################################################################################
#
# Kinship matrices
#


### The following lines generate the kinship matrix from the GTEx genotypes, using GCTA:
#
# Before QC:
# /home/nasheran/cozygene/software/GCTA/gcta64 --bfile /home/nasheran/schweiger/backup/lmm/data/GTEX/phg000219.v3.GTEx_Illumina5M.genotype-calls-matrixfmt.c1/GTEX_Omni5_191 --autosome --make-grm-bin --out /home/nasheran/schweiger/backup/lmm/data/matrices/gtex_kinship_before_qc
#
# After QC:
# /home/nasheran/cozygene/software/GCTA/gcta64 --bfile /home/nasheran/schweiger/backup/lmm/data/GTEX/phg000219.v3.GTEx_Illumina5M.genotype-calls-matrixfmt.c1/GTEX_Omni5_191 --remove /home/nasheran/schweiger/backup/lmm/data/GTEX/phg000219.v3.GTEx.genotype-qc.MULTI/GTEX_Illumina5M_genotypes/GTEx_5M.sample.exclusion_with_keinfelter.txt --exclude /home/nasheran/schweiger/backup/lmm/data/GTEX/phg000219.v3.GTEx.genotype-qc.MULTI/GTEX_Illumina5M_genotypes/5M.exclusion.snplist.dbgap.txt --autosome --make-grm-bin --out /home/nasheran/schweiger/backup/lmm/data/matrices/gtex_kinship_after_qc
#
### The following lines generate the kinship matrix from the LURIC genotypes, using GCTA:
#
# /home/nasheran/cozygene/software/GCTA/gcta64 --bfile /home/nasheran/cozygene/d/athero/LURIC-LIPIDS/LURIC_AFFY_FINAL_clean --keep /home/nasheran/schweiger/backup/lmm/data/matrices/luric_samples_with_lipids_to_keep.txt --out /home/nasheran/schweiger/backup/lmm/data/matrices/luric_kinship_after_qc --autosome --make-grm-bin
# 
# The luric_samples_with_lipids_to_keep.txt is a file containing the IDs of samples of which we have lipid data.
# 
### The NFBC matrix (filtered) was given to me by Serghei at Zar's lab.


def create_gcta_kinship_matrix(filename_prefix, verbose=False):
    """
    Create a kinship matrix, identical to the one generated by GCTA with the --autosome flag.

    Summary:
    - Standardize each SNP, according to its nonmissing values, by w_ij = (x_ij - p)/(sqrt(2*p*(1-p)))
    - The kinship matrix at i,j is the dot product for the i,j samples, over all SNPs that are nonmissing
      in both, divided by the number of such SNPs.

    If there are no missing SNPs, this is equivalent to 1/n_snps * W.T * W.

    Arguments:
        filename_prefix - Filename of PLINK file (we need *.bed and *.map)
    """
    # Create a list of all autosomal SNPs
    if verbose:
        print "Building list of autosomal SNPs..."
    plink_map_file = file(filename_prefix + ".map")
    autosomal_snps = set([])
    max_snp = 0
    for line in plink_map_file:
        chromosome_number, snp_name = line.split()[:2]
        if int(chromosome_number) <= 22:
            autosomal_snps.add(snp_name)
            max_snp += 1
        else:  # REMOVE
            break

    n_snps = len(autosomal_snps)
    if verbose:
        print "Selecting %d SNPs" % n_snps

    # Build a matrix of unnormalized genotypes, for the autosomal SNPs
    if verbose:
        print "Reading genotypes..."

    plink_bed_file = pylmm.input.plink(filename_prefix, type="b", normGenotype=False)
    n_samples = len(plink_bed_file.indivs)

    X = zeros((n_snps, n_samples), dtype=float32)
    plink_bed_file_iterator = plink_bed_file.getSNPIterator()

    current_line = 0
    current_snp = 0
    for snp_data, snp_name in plink_bed_file_iterator:
        if verbose and (current_line % 100000 == 0):
            print "Reading SNP %d..." % current_snp
        if snp_name in autosomal_snps:
            # pylmm reads plink file as [0,0.5,1]. Convert to [0,1,2] to comply with GCTA's formulation.
            X[current_line,:] = 2*snp_data
            current_line += 1
        current_snp += 1
        if current_snp > max_snp:  # REMOVE
            break

    # Remember where were the missing values and set these to 0
    if verbose:
        "Mapping missing values..."
    missing = isnan(X)
    missing_sparse = scipy.sparse.csc_matrix(missing)
    X[missing] = 0

    # Calculate the frequency and HW std for each SNP, based only on known values
    if verbose:
        "Calculating SNP frequencies..."
    mus = sum(X, 1) / (n_samples - array(missing_sparse.sum(1)).T[0])
    mus[isnan(mus)] = 0
    ss = mus*(1-0.5*mus)
    ss[ss != 0] = 1/(ss[ss != 0] ** 0.5)

    # Standardize SNPs
    if verbose:
        print "Standardizing SNPs..."
    normX = (X - mus[:,newaxis]) * ss[:,newaxis]
    normX[missing] = 0

    # Calculate the number of actual nonmissing SNPs in both pair of samples, for each pair of samples
    if verbose:
        print "Calculating nonmissing SNPs count for each pair..."
    N = zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(i+1):
            N[i,j] = N[j,i] = n_snps - missing_sparse[:,i].maximum(missing_sparse[:,j]).getnnz()

    # Calculate the final matrix
    if verbose:
        print "Calculating kinship matrix..."

    kinship = dot(normX.T, normX) / N

    if verbose:
        print "Done!"

    return kinship



def load_gcta_kinship_matrix_file(filename, n_samples):
    """
    Load a kinship matrix, coded in binary format by GCTA.

    Arguments:
        filename - Filename of the matrix (*.grm.bin)
        n_samples - Number of samples, i.e. the size of the matrix (this is the number of lines in the corresponding *.grm.id file)

    """
    mat = zeros(shape=(n_samples, n_samples), dtype=float32)
    lower_mat = fromfile(filename, dtype=float32)
    mat[tril_indices(n_samples)] = lower_mat
    mat = mat + mat.T
    mat[diag_indices(n_samples)] = mat[diag_indices(n_samples)]/2
    return mat




################################################################################################################
#
# Heritability estimates of real phenotypes
#

# For GTEX
# TO FINISH!
WHOLEBLOOD_EXPRESSION_FILENAME = "/home/nasheran/schweiger/backup/lmm/data/heritability/GTEx/expression/Whole_Blood.expr.txt"

def real_gtex_expression_profiles(ids_filename, phenotypes_filename):
    ids = [l.strip().split()[1] for l in file(ids_filename).readlines()]

    reader = csv.reader(file(phenotypes_filename), delimiter='\t')
    genes = next(reader)

    return ids, genes

# For LURIC
LURIC_DIR = "/home/nasheran/schweiger/backup/ewas/data/LURIC"
PHENOTYPE_FILE = "luric_lipids_table_19122012_for_analysis.txt"

# TO FINISH!

def luric_get_all_phenotypes(id_filename = "/home/nasheran/schweiger/backup/lmm/data/matrices/luric_kinship_after_qc.grm.id", 
                             phenotype_filename = os.path.join(LURIC_DIR, PHENOTYPE_FILE)):
    ids = [int(l.strip().split()[1]) for l in file(id_filename).readlines()]

    first = True
    columns = []
    phenotypes = []
    phenoids = []

    for l in file(phenotype_filename).readlines():
        s = l.strip().split('\t')
        if first:
            phenotypes = s[2:]          
            columns = {p:[0]*len(ids) for p in phenotypes}
            first = False
            continue

        if int(s[1]) in ids:            
            phenoids.append(s[1])           
            for i, x in enumerate(s[2:]):
                if x in ['NA', '']:
                    x = 0.0
                else:
                    x = float(x)
                columns[phenotypes[i]][ids.index(int(s[1]))] = x
    return columns

################################################################################################################
#
# Linear algbera
#

def sorted_eig(A, totype=float):
    """
    Sorted eigenvalues and eigenvectors, return the eigenvectors with their first coordinate 
    as positive for uniqueness.

    Arguments:
        A - matrix to decompose.
        totype - type to convert to (or None).

    Returns eigenvalues and eigenvectors, like numpy.linalg.eig.
    """
    eigenValues, eigenVectors = numpy.linalg.eig(A)

    idx = eigenValues.argsort()[::-1]   
    eigenValues = eigenValues[idx]
    eigenVectors = eigenVectors[:,idx]
    eigenVectors *= (-1)**(eigenVectors[0,:]<0)
    if totype is not None:
        return eigenValues.astype(totype), eigenVectors.astype(totype)
    else:
        return eigenValues, eigenVectors









################################################################################################################
#
# Probability of h^2 estimator
#
def multiply_matrices(matrix_list):
    return reduce(lambda x, y: dot(x,y), matrix_list)

def weights_zero_derivative(true_h2, estimated_h2, kinship_eigenvalues, 
                            eigenvectors_as_X=[-1], REML=True):
    """
    Calculates the weights in the expression of the derivative of the likelihood, in the case that
    X is eigenvectors of the kinship matrix. If multiple estimated_h2 are given, this is a matrix
    of the weights for each required estimated_h2.
    """
    if eigenvectors_as_X is None:
        eigenvectors_as_X = []
    if isinstance(true_h2, int) or isinstance(true_h2, float):
        true_h2 = [true_h2]
    if isinstance(estimated_h2, int) or isinstance(estimated_h2, float):
        estimated_h2 = [estimated_h2]
    n_samples = len(kinship_eigenvalues)

    estimated_h2 = reshape(estimated_h2, (1, len(estimated_h2), 1))
    true_h2 = reshape(true_h2, (len(true_h2), 1, 1))
    kinship_eigenvalues = reshape(kinship_eigenvalues, (1, 1, n_samples))

    projection = ones((1, 1, n_samples))
    projection[0, 0, eigenvectors_as_X] = 0
    
    ds = (kinship_eigenvalues - 1) / (estimated_h2 * (kinship_eigenvalues - 1) + 1)
    denom = n_samples

    if REML:
        ds = projection * (kinship_eigenvalues - 1) / (estimated_h2 * (kinship_eigenvalues-1) + 1)
        denom = n_samples - len(eigenvectors_as_X)

    return projection * (true_h2 * (kinship_eigenvalues - 1) + 1) / \
                        (estimated_h2 * (kinship_eigenvalues - 1) + 1) \
                                      * (ds - sum(ds, 2)[:, :, newaxis] / denom)


def calculate_probability_intervals(true_h2, interval_boundaries, kinship_eigenvalues, 
                                    eigenvectors_as_X=[-1], REML=True, monte_carlo_size=1000, n_chunks=1, seed=0):
    n_samples = len(kinship_eigenvalues)
    n_intervals = len(interval_boundaries)-1

    # Size: n_true_h2 X n_grid X n_individuals
    weights = weights_zero_derivative(true_h2, interval_boundaries, kinship_eigenvalues, eigenvectors_as_X=eigenvectors_as_X, REML=REML)
    
    rng = numpy.random.RandomState(seed)
    prob = zeros((len(true_h2), n_intervals+2))

    for i in itrange(n_chunks):
        # Avoid replicating weights across h2's, so that each sample is independent
        us = rng.normal(size=(monte_carlo_size, len(true_h2), 1, n_samples))
        dotproducts = sum((us**2) * weights[newaxis, :, :, :], 3)     
        
        #prob[:, 1:-1] += mean((dotproducts[:, :, :-1] >= 0) & (dotproducts[:, :, 1:] <= 0), 0)
        #prob[:, 0] += mean(dotproducts[:, :, 0] <= 0, 0)        
        #prob[:, -1] += mean(dotproducts[:, :, -1] >= 0, 0) 
        
        # Size: monte_carlo_size X len(true_h2)
        hit_zero = (dotproducts[:, :, 0] <= 0)
        hit_one = (dotproducts[:, :, -1] >= 0)
        hit_boundary = hit_zero | hit_one

        # Size: len(true_h2)
        prob[:, 0] += mean(hit_zero, 0)        
        prob[:, -1] += mean(hit_one, 0)        
        prob[:, 1:-1] += mean(((dotproducts[:, :, :-1] >= 0) & (dotproducts[:, :, 1:] <= 0)) & ~hit_boundary[:, :, newaxis], 0)
        
    prob /= n_chunks       # Average across chunks
    prob /= sum(prob, 1)[:, newaxis]   # Normalize so sum is 1, in case of multiple local maxima
    return prob


def average_information_matrix_spectral_intercept(sigma_g, sigma_e, ev, rotated_y):
    
    P = 1/(sigma_g * ev + sigma_e)
    P[-1] = 0

    AI = zeros((2,2))
    rotated_y2 = rotated_y**2
    AI[0,0] = dot(P**3 * ev**2, rotated_y2) 
    AI[0,1] = dot(P**3 * ev**1, rotated_y2)
    AI[1,0] = dot(P**3 * ev**1, rotated_y2)
    AI[1,1] = dot(P**3, rotated_y2)

    AI *= 0.5

    AI = numpy.linalg.inv(AI)

    return AI   

def information_matrix_to_heritability_variance(AI, sigma_g, sigma_e):
    sum_sigmas = sigma_g + sigma_e
    var_h = (AI[0,0] * (sigma_e ** 2) - 2 * AI[0,1] * (sigma_e * sigma_g) + AI[1,1] * (sigma_g**2)) / (sum_sigmas**4)

    return var_h

def average_information_spectral_var(h2, sigma_p, ev, rotated_y):
    sigma_g = h2 * sigma_p
    sigma_e = (1-h2) * sigma_p
    return information_matrix_to_heritability_variance(
                average_information_matrix_spectral_intercept(sigma_g, sigma_e, ev, rotated_y),
                sigma_g, sigma_e)


def calculate_estimates_and_std(true_h2, interval_boundaries, kinship_eigenvalues, 
              eigenvectors_as_X=[-1], REML=True, monte_carlo_size=1000, n_chunks=1, seed=0, return_us=False):
    # Process parameters
    if isinstance(true_h2, int) or isinstance(true_h2, float):
        true_h2 = [true_h2]
    true_h2 = array(true_h2)

    n_individuals = len(kinship_eigenvalues)
    n_true_h2 = len(true_h2)
    total_monte_carlo_size = monte_carlo_size * n_chunks
    
    if eigenvectors_as_X is None:
        eigenvectors_as_X = []
    p = ones(n_individuals)
    p[eigenvectors_as_X] = 0

    rng = numpy.random.RandomState(seed)        

    interval_midpoints = (interval_boundaries[:-1]+interval_boundaries[1:])/2

    all_us = []
    all_dotproducts = []
    
    # Create results
    # Size: n_true_h2 X monte_carlo_size X n_chunks X 2
    data = zeros((n_true_h2, monte_carlo_size, n_chunks, 2))   

    for n_chunk in itrange(n_chunks): 
        # Size: n_true_h2 X monte_carlo_size X 2
        #res = zeros((n_true_h2, monte_carlo_size, 2))    
        res = data[:, :, n_chunk, :]

        # Size: n_true_h2 X monte_carlo_size X n_individuals
        us = rng.randn(n_true_h2, monte_carlo_size, n_individuals)
        if return_us:
            all_us.append(us)

        # Size: n_true_h2 X n_grid X n_individuals
        weights = weights_zero_derivative(true_h2, interval_boundaries, kinship_eigenvalues, 
                                          eigenvectors_as_X=eigenvectors_as_X, REML=REML)

        # Size: n_true_h2 X monte_carlo_size X n_individuals
        rotated_ys = (true_h2[:, newaxis, newaxis] * (kinship_eigenvalues[newaxis, newaxis, :] - 1) + 1)**0.5 * us

        # Size: n_true_h2 X monte_carlo_size X n_grid
        ## THIS IS THE MEMORY-HEAVIEST PART, unroll
        dotproducts = zeros((n_true_h2, monte_carlo_size, shape(weights)[1]))
        for i in range(n_true_h2):
            # Size: monte_carlo_size X n_grid
            dotproducts[i, :, :] = sum(us[i, :, newaxis, :]**2 * weights[i, newaxis, :, :], 2)
        #dotproducts = sum(us[:, :, newaxis, :]**2 * weights[:, newaxis, :, :], 3)
        if return_us:
            all_dotproducts.append(dotproducts)

        xs, ys, zs = where((dotproducts[:, :, :-1] >= 0) & (dotproducts[:, :, 1:] <= 0))

        # Size: n_true_h2 X monte_carlo_size
        h2_estimates = zeros((n_true_h2, monte_carlo_size))

        h2_estimates[xs, ys] = interval_midpoints[zs] 
        h2_estimates[dotproducts[:, :,  0] <= 0] = 0.0        
        h2_estimates[dotproducts[:, :, -1] >= 0] = 1.0

        # Size: n_true_h2 X monte_carlo_size X n_individuals (depends on h2 estimate)
        var_weights_nominators   = true_h2[:, newaxis, newaxis] * (kinship_eigenvalues[newaxis, newaxis, :] - 1) + 1
        var_weights_denominators = h2_estimates[:, :, newaxis] * (kinship_eigenvalues[newaxis, newaxis, :] - 1) + 1
        var_weights = p[newaxis, newaxis, :] * var_weights_nominators / var_weights_denominators \
                                             / (n_individuals - len(eigenvectors_as_X))

        # Size: n_true_h2 X monte_carlo_size
        sigma_p2_estimates = sum(var_weights * (us**2), 2)

        # Fill results
        res[:, :, 0] = h2_estimates
        for i in range(n_true_h2):
            for j in range(monte_carlo_size):    
                res[i, j, 1] = average_information_spectral_var(h2_estimates[i, j], 
                                                                sigma_p2_estimates[i, j], 
                                                                kinship_eigenvalues, 
                                                                rotated_ys[i, j, :]) ** 0.5

    # Unroll chunks and return
    data = reshape(data, (n_true_h2, total_monte_carlo_size, 2))
    
    if return_us:
        return data, all_us, all_dotproducts
    else:
        return data

def get_approx_inv_cdf(true_h2s, cdf, alpha):
    """
    Find a value c that satisfies Pr(X <= c) >= alpha.

    Delicate because of the discontinuity points at 0,1. More docs here!!!
    """
    epsilon = 1e-10

    # max{x | P(X <= x) == p_0}
    L = bisect.bisect_right(cdf, cdf[0])-1

    # min{x | P(X <= x) == 1-p_1}
    R = bisect.bisect_left(cdf, cdf[-2])

    # Now, the open interval (true_h2s[L], true_h2s[R]) maps 1-1 to the open interval (p0, 1-p1)
    if (cdf[0] < alpha < cdf[-2]):
        subrange = true_h2s[L:R+1]
        subcdf = cdf[L:R+1]
        idx = bisect.bisect_left(subcdf, alpha)
        if subcdf[idx] == alpha: # If we hit exactly right, simplify things and just return that
            return subrange[idx]
        else:
            # Otherwise, the required value is explicity inside the interval (subcdf[idx-1], subcdf[idx]),
            # so to get the right value, interpolate linearly
            proportion = (alpha - subcdf[idx-1]) / (subcdf[idx] - subcdf[idx-1])
            value = subrange[idx-1] + proportion * (subrange[idx] - subrange[idx-1])
            return value

    # If it's not in that open interval, this either alpha <= p0 or alpha >= 1-p1
    elif (alpha <= cdf[0]):
        if not numpy.isclose(alpha, cdf[0]):   # Should have been (alpha < cdf[0]). Impossible
            #return -inf
            return 0.0
        else:  # alpha == p0, and we want to take the largest of these values
            return true_h2s[L]
    elif (alpha >= cdf[-2]):
        if alpha >= cdf[-1] or numpy.isclose(alpha, cdf[-1]):   
            return 1.0
        else:
            return 1.0 - epsilon
    else:
        assert "Should not happen"

def get_approx_cdf(true_h2s, cdf, real_value):
    """
    Find P(X <= real_value)
    """
    assert 0 <= real_value <= 1
    if real_value == 0.0:
        return cdf[0]
    elif real_value == 1:
        return 1.0
    else:
        idx = bisect.bisect_left(true_h2s, real_value)
        if true_h2s[idx] == real_value:
            return cdf[idx]
        else:        
            proportion = (real_value - true_h2s[idx-1]) / (true_h2s[idx] - true_h2s[idx-1])
            return cdf[idx-1] + (cdf[idx] - cdf[idx-1]) * proportion 

def get_probabilty_in_closed_interval(true_h2s, cdf, interval):
    """
    Pr(X in [interval[0], interval[1]])
    """
    p = get_approx_cdf(true_h2s, cdf, interval[1]) - get_approx_cdf(true_h2s, cdf, interval[0])
    if interval[0] == 0.0:
        p += cdf[0]
    return p

def build_acceptance_region_given_distribution(true_h2s, distribution, real_value, alpha, verbose=False):
    numerical_error = 1e-5
    epsilon = 1e-10   # Small value that is just above 0 but still lower than all actual values probably

    cdf = cumsum(distribution)
    
    # Pr(\hat{h2} = 0)
    p0 = distribution[0]

    # Pr(\hat{h2} = 1)
    p1 = distribution[-1]

    # Pr(0 < \hat{h2} <= h2)
    pl = get_approx_cdf(true_h2s, cdf, real_value) - p0

    # Pr(h2 < \hat{h2} < 1)
    pr = 1 - p1 - p0 - pl
    
    if verbose: print p0, pl, pr, p1

    # First suggestion: Go left from real_value, until hitting zero, then go right. This
    # is always guaranteed to cover alpha, but may be significantly more than alpha.
    if (p0 + pl) >= alpha:
        low, high = get_approx_inv_cdf(true_h2s, cdf, pl + p0 - alpha), real_value
        if low == -inf:
            low = 0.0
    elif (p0 + pl + pr) >= alpha:
        low = 0.0
        high = get_approx_inv_cdf(true_h2s, cdf, alpha)
    else:    # you have to take all of it, since dropping 0 or 1 will give you less that the CI
        low = 0.0
        high = 1.0
    coverage = get_probabilty_in_closed_interval(true_h2s, cdf, [low, high])

    low1, high1, coverage1 = low, high, coverage
    if verbose: print low, high, coverage

    # Second suggestion, drop 0 and take the best possible given that. 
    if True or (pl + pr + p1) >= alpha:
        low2 = get_approx_inv_cdf(true_h2s, cdf, p0)
        if low2 == 0.0:
            low2 = epsilon
        high2 = get_approx_inv_cdf(true_h2s, cdf, alpha + p0)
        coverage2 = get_probabilty_in_closed_interval(true_h2s, cdf, [low2, high2])

        if verbose: print low2, high2, coverage2
        if (alpha <= coverage2 + numerical_error) and \
           ((coverage2 + numerical_error <= coverage) or \
            (abs(coverage - coverage2) <= numerical_error and abs(high - high2) < numerical_error)):  
            low, high = low2, high2
            coverage = coverage2

        # Third suggestion, take 1 and take the best possible given that. 
        low3 = get_approx_inv_cdf(true_h2s, cdf, 1 - alpha)
        high3 = 1.0
        coverage3 = get_probabilty_in_closed_interval(true_h2s, cdf, [low3, high3])        
        if (alpha <= coverage3 + numerical_error) and (coverage3 + numerical_error <= coverage):
            low, high = low3, high3
            coverage = coverage3

        if verbose: print low3, high3, coverage3

    #return [low1, high1, coverage1], [low2, high2, coverage2], [low3, high3, coverage3]
    return low, high, coverage


def build_khaper_acceptance_region_given_distribution(true_h2s, distribution, real_value, alpha, verbose=False):
    numerical_error = 1e-5
    epsilon = 1e-10   # Small value that is just above 0 but still lower than all actual values probably

    cdf = cumsum(distribution)
    
    # Pr(\hat{h2} = 0)
    p0 = distribution[0]

    # Pr(\hat{h2} = 1)
    p1 = distribution[-1]

    # Pr(0 < \hat{h2} <= h2)
    pl = get_approx_cdf(true_h2s, cdf, real_value) - p0

    # Pr(h2 < \hat{h2} < 1)
    pr = 1 - p1 - p0 - pl
    
    if verbose: print p0, pl, pr, p1

    # First suggestion: [0, max(true_value, c_alpha)]
    low1 = 0.0
    high1 = max(real_value, get_approx_inv_cdf(true_h2s, cdf, alpha))
    if high1 == 1 - epsilon:
        high1 = 1.0
    coverage1 = get_probabilty_in_closed_interval(true_h2s, cdf, [low1, high1])

    # Second suggestion: a symmetric open interval around the true_value, extended until
    # either it reaches alpha, or reaches the boundaries, but not including them (up to (0,1))
    if pr > alpha/2 and pl > alpha/2:
        low2 = get_approx_inv_cdf(true_h2s, cdf, p0 + pl - alpha/2)
        high2 = get_approx_inv_cdf(true_h2s, cdf, p0 + pl + alpha/2)
    elif pr > alpha/2 and pl <= alpha/2:
        low2 = 0.0 #epsilon
        high2 = get_approx_inv_cdf(true_h2s, cdf, p0 + alpha)
    elif pr <= alpha/2 and pl > alpha/2:
        low2 = get_approx_inv_cdf(true_h2s, cdf, 1 - p1 - alpha)
        high2 = 1.0 - epsilon
    else:   # pr <= alpha/2 and pl <= alpha/2
        low2 = 0.0 #epsilon
        high2 = 1.0 - epsilon

    low2 = epsilon
    high2 = 1.0 
    coverage2 = get_probabilty_in_closed_interval(true_h2s, cdf, [low2, high2])        

    # Third suggestion, [min(true_value, c_{1-alpha}), 1]
    low3 = min(real_value, get_approx_inv_cdf(true_h2s, cdf, 1 - alpha))
    high3 = 1.0
    coverage3 = get_probabilty_in_closed_interval(true_h2s, cdf, [low3, high3])        

    return [low1, high1, coverage1], [low2, high2, coverage2], [low3, high3, coverage3]

def build_heritability_cis(accept_regions, true_h2s, values):        
    starts = [a[0] for a in accept_regions]
    ends = [a[1] for a in accept_regions]
    cis = zeros((len(values), 2))
    for i,estimated_h in enumerate(values):
        #print estimated_h, ends, bisect.bisect_left(ends, estimated_h)
        idx = bisect.bisect_left(ends, estimated_h)
        if idx == 0:
            cis[i,0] = 0.0
        else:
            proportion = (estimated_h - ends[idx-1]) / (ends[idx] - ends[idx-1])
            cis[i,0] = true_h2s[idx-1] + (true_h2s[idx] - true_h2s[idx-1]) * proportion 
        
        idx = bisect.bisect_right(starts, estimated_h)
        if idx == len(true_h2s):
            cis[i,1] = 1.0
        else:
            proportion = (estimated_h - starts[idx-1]) / (starts[idx] - starts[idx-1])
            cis[i,1] = true_h2s[idx-1] + (true_h2s[idx] - true_h2s[idx-1]) * proportion 
    return cis

def build_khaper_heritability_cis(khaper_accept_regions, true_h2s, values, alpha, prob0, seed=0):  
    khaper_accept_regions = array(khaper_accept_regions)      
    ends = maximum.accumulate(khaper_accept_regions[:,0,1])
    starts = maximum.accumulate(khaper_accept_regions[:,2,0])

    starts[ends < 1] = 0

    A = khaper_accept_regions[:,0,2]
    B = A - prob0
    
    prob_interval = (alpha - B) / (A - B)
    prob_interval[isclose(khaper_accept_regions[:,2,2], alpha)] = 0
    prob_interval[isclose(A, alpha)] = 1
    prob_interval = minimum.accumulate(prob_interval)
    

    rng = numpy.random.RandomState(seed)


    cis = zeros((len(values), 2))
    for i,estimated_h in enumerate(values):
        if estimated_h == 0.0:
            # special case, random confidence interval
            cis[i, 0] = 0.0
            rand_value = rng.rand()
            cis[i, 1] = true_h2s[where(rand_value < prob_interval)[0][-1]]
            continue

        #print estimated_h, ends, bisect.bisect_left(ends, estimated_h)
        idx = bisect.bisect_left(ends, estimated_h)
        if idx == 0:
            cis[i,0] = 0.0
        else:
            proportion = (estimated_h - ends[idx-1]) / (ends[idx] - ends[idx-1])
            cis[i,0] = true_h2s[idx-1] + (true_h2s[idx] - true_h2s[idx-1]) * proportion 
        
        idx = bisect.bisect_right(starts, estimated_h)
        if idx == len(true_h2s):
            cis[i,1] = 1.0
        else:
            proportion = (estimated_h - starts[idx-1]) / (starts[idx] - starts[idx-1])
            cis[i,1] = true_h2s[idx-1] + (true_h2s[idx] - true_h2s[idx-1]) * proportion 
    return cis    

def estimate_accuracy_of_heritability_cis(true_heritability_value, heritability_estimates, accept_regions, true_h2s):
    return mean([(ci[0] <= true_heritability_value <= ci[1]) for ci in build_heritability_cis(accept_regions, true_h2s, heritability_estimates)])


################################################################################################################
#
# Benchmarks
#


# Draw quickly from a normal multivariate distribution ~N(0, X*X^T) given X
def draw_multivariate(X, times=1, random_seed=None):
    return dot(X, numpy.random.RandomState(random_seed).randn(shape(X)[0], times))

def draw_multivariate_from_eigen(U, eigvals, times=1, random_seed=None):
    return draw_multivariate(dot(U, diag(maximum(eigvals, 0)**0.5)), times, random_seed)

GCTA_BINARY = "/specific/a/nas-eran-01/groups/eran_halperin/cozygene/backup/software/GCTA/gcta64"

def run_gcta_reml(pheno_filename, grm_prefix, gcta_binary=GCTA_BINARY, additional_flags=""):
    out_filename = tempfile.NamedTemporaryFile().name
    command_line = "%s --out %s --grm-bin %s --reml --pheno %s --reml-alg 0 --reml-maxit 1000 %s" % (gcta_binary, out_filename, grm_prefix, pheno_filename, additional_flags)
    subprocess.check_output(command_line, shell=True)
    
    out_filename = out_filename + ".hsq"
    try:
        output = file(out_filename).readlines()

        d = {}
        for line in output[1:]:
            x = line.strip().split()
            d[x[0]] = map(float, x[1:])
        os.remove(out_filename)
    except:
        return {}
    return d


def run_gcta_reml_phenotype(pheno_array, grm_prefix, gcta_binary=GCTA_BINARY, additional_flags=""):
    id_file = grm_prefix + ".grm.id"
    ids = [x.strip() for x in file(id_file).readlines()]
    assert len(ids) == len(pheno_array)
    pheno_filename = tempfile.NamedTemporaryFile().name
    
    f = file(pheno_filename, "w")
    for i in range(len(ids)):
        f.write(ids[i] + " " + str(pheno_array[i]) + "\n")
    f.close()
    
    d = run_gcta_reml(pheno_filename, grm_prefix, gcta_binary)
    os.remove(pheno_filename)
    return d

###

def time_distribution_generation(interval_boundaries, kinship_eigenvalues, runs=100):
    start_time = time.time()
    calculate_probability_intervals([0.5], interval_boundaries, kinship_eigenvalues, 
                                    [-1], True, 1, runs, 0)
    end_time = time.time()
    return end_time - start_time


def time_distribution_pylmm(kinship_eigenvalues, kinship_eigenvectors, runs=100):
    start_time = time.time()
    for h2 in [0.5]:
        ys = draw_multivariate_from_eigen(kinship_eigenvectors, h2*kinship_eigenvalues + (1-h2), runs, 0)
        dummy = [pylmm.lmm_unbounded.LMM(ys[:, i], kinship_eigenvectors, kinship_eigenvalues, kinship_eigenvectors, 
                                         X0=ones((len(kinship_eigenvalues), 1))).fit(REML=True, explicit_H=arange(0,1.01,0.01)) \
                    for i in range(runs)]
    end_time = time.time()
    return end_time - start_time     

def time_distribution_gcta(kinship_eigenvalues, kinship_eigenvectors, grm_prefix, runs=100):
    start_time = time.time()
    successes = []
    for h2 in [0.5]:
        ys = draw_multivariate_from_eigen(kinship_eigenvectors, h2*kinship_eigenvalues + (1-h2), runs, 0)
        dummy = [run_gcta_reml_phenotype(ys[:,i], grm_prefix) for i in range(runs)]
        successes.append(sum([len(d) > 0 for d in dummy]))
    end_time = time.time()
    return end_time - start_time, successes

################################################################################################################
#
# Plots of paper
#

WHERE = "tau"
if socket.gethostname().split('.')[-1] in ['local', 'EDU']:
    WHERE = "local"

MATRIX_DECOMPOSITION_DIR = ("/Users/regevschweiger/git/cozygene/ewas/matrices/" if WHERE == "local" else "/specific/a/home/cc/students/cs/schweiger/git/cozygene/ewas/matrices")
OUTPUT_DIR = "/Users/regevschweiger/Google Drive/University/TAU/LMM/paper/"

def load_all_eigenvalues():
    global gtex_eigenvalues, luric_eigenvalues, nfbc_eigenvalues, all_eigenvalues
    if any([s not in globals() for s in ["gtex_eigenvalues", "luric_eigenvalues", "nfbc_eigenvalues", "load_all_eigenvalues"]]):
        dummy, gtex_eigenvalues = cPickle.load(file(os.path.join(MATRIX_DECOMPOSITION_DIR, "gtex_after_qc_matrix_decomposition.pcl"), "rb"))
        dummy, luric_eigenvalues = cPickle.load(file(os.path.join(MATRIX_DECOMPOSITION_DIR, "luric_matrix_decomposition.pcl"), "rb"))
        dummy, nfbc_eigenvalues = cPickle.load(file(os.path.join(MATRIX_DECOMPOSITION_DIR, "nfbc_filtered_matrix_decomposition.pcl"), "rb"))
        all_eigenvalues = [gtex_eigenvalues, luric_eigenvalues, nfbc_eigenvalues]
        for i,ev in enumerate(all_eigenvalues):
            all_eigenvalues[i] = maximum(ev, 1e-10)

# Global constants
h2_step = 0.01
h2_range = arange(0, 1+h2_step, h2_step)

fine_h2_step = 0.001
fine_h2_range = arange(0, 1+fine_h2_step, fine_h2_step)

confidence_levels = [0.7, 0.8, 0.9, 0.95]


def create_estimator_distribution(eigenvalues, output_filename):    
    all_distributions = calculate_probability_intervals(fine_h2_range, h2_range, eigenvalues, eigenvectors_as_X=[-1], 
                                                        REML=True, monte_carlo_size=1, n_chunks=10000, seed=0)
    writer = csv.writer(file(output_filename, "w"), delimiter='\t')
    writer.writerow(["h2"] + ["%.3f" % h2 for h2 in fine_h2_range])
    writer.writerows(hstack([array([-h2_step]+list(h2_range))[:,newaxis], array(all_distributions).T]))

def load_distribution(filename):
    return array(list(csv.reader(file(filename, "r"), delimiter='\t'))[1:])[:,1:].astype(float)

def create_all_estimator_distributions():
    load_all_eigenvalues()
    create_estimator_distribution(gtex_eigenvalues,     os.path.join(OUTPUT_DIR, "gtex_h2_distribution.dat"))
    create_estimator_distribution(luric_eigenvalues,    os.path.join(OUTPUT_DIR, "luric_h2_distribution.dat"))
    create_estimator_distribution(nfbc_eigenvalues,     os.path.join(OUTPUT_DIR, "nfbc_filtered_h2_distribution.dat"))

def create_all_estimator_and_std_samples():
    print "Warning, this might take long! PCLs available"
    load_all_eigenvalues()

    cPickle.dump(calculate_estimates_and_std(fine_h2_range, h2_range, gtex_eigenvalues, 
                        eigenvectors_as_X=[-1], REML=True, monte_carlo_size=100, n_chunks=100, seed=0),
                 file(os.path.join(OUTPUT_DIR, "gtex_est_stds_1000_10000.pcl"), "wb"))

    cPickle.dump(calculate_estimates_and_std(fine_h2_range, h2_range, luric_eigenvalues, 
                        eigenvectors_as_X=[-1], REML=True, monte_carlo_size=100, n_chunks=100, seed=0),
                 file(os.path.join(OUTPUT_DIR, "luric_est_stds_1000_10000.pcl"), "wb"))

    cPickle.dump(calculate_estimates_and_std(fine_h2_range, h2_range, nfbc_eigenvalues, 
                        eigenvectors_as_X=[-1], REML=True, monte_carlo_size=100, n_chunks=100, seed=0),
                 file(os.path.join(OUTPUT_DIR, "nfbcfil_est_stds_1000_10000.pcl"), "wb"))






# Actual plots

def plot_probability_boundary(output_filename=os.path.join(OUTPUT_DIR, "probability_boundary.dat")):    
    export = []
    export.append(fine_h2_range)

    for filename in ["gtex_h2_distribution.dat", "luric_h2_distribution.dat", "nfbc_filtered_h2_distribution.dat"]:
        lines = list(csv.reader(file(os.path.join(OUTPUT_DIR, filename), "r"), delimiter='\t'))
        prob0 = array(lines[1][1:]).astype(float)
        prob1 = array(lines[-1][1:]).astype(float)
        export.extend([prob0, prob1])

    writer = csv.writer(file(output_filename, "w"), delimiter='\t')
    writer.writerow(["h2", "gtex_prob0", "gtex_prob1", "luric_prob0", "luric_prob1", "nfbc_prob0", "nfbc_prob1"])
    writer.writerows(array(export).T)


def plot_bias(output_filename=os.path.join(OUTPUT_DIR, "bias.dat")):
    estimated_values = array([0]+list((h2_range[:-1]+h2_range[1:])/2)+[1])

    export = []
    export.append(fine_h2_range)

    for filename in ["gtex_h2_distribution.dat", "luric_h2_distribution.dat", "nfbc_filtered_h2_distribution.dat"]:
        lines = list(csv.reader(file(os.path.join(OUTPUT_DIR, filename), "r"), delimiter='\t'))
        bias = dot(estimated_values, array(lines[1:]).astype(float)[:,1:]) - fine_h2_range
        export.append(bias)

    writer = csv.writer(file(output_filename, "w"), delimiter='\t')
    writer.writerow(["h2", "gtex_bias", "luric_bias", "nfbc_bias"])
    writer.writerows(array(export).T)


def plot_std_ratio(output_filename=os.path.join(OUTPUT_DIR, "std_ratio.dat")):
    export = []
    export.append(fine_h2_range)

    for filename in ["gtex_est_stds_1000_10000.pcl", "luric_est_stds_1000_10000.pcl", "nfbcfil_est_stds_1000_10000.pcl"]:
        data = cPickle.load(file(os.path.join(OUTPUT_DIR, filename), "rb"))
        export.append(mean(data[:, :, 1], 1) / std(data[:, :, 0], 1))

    writer = csv.writer(file(output_filename, "w"), delimiter='\t')
    writer.writerow(["h2", "gtex_std_ratio", "luric_std_ratio", "nfbc_std_ratio"])
    writer.writerows(array(export).T)

def plot_gcta_cis(output_filename=os.path.join(OUTPUT_DIR, "gcta_cis.dat")):    
    def value_in_interval_vectorized(true_h2, estimated_h2, estimated_std, stds=1):
        return ((estimated_h2 - estimated_std * stds) <= true_h2) &  \
               ((estimated_h2 + estimated_std * stds) >= true_h2)

    export = []
    export.append(fine_h2_range)

    coverages = []
    prob0 = []
    for filename in ["gtex_est_stds_1000_10000.pcl", "luric_est_stds_1000_10000.pcl", "nfbcfil_est_stds_1000_10000.pcl"]:
        data = cPickle.load(file(os.path.join(OUTPUT_DIR, filename), "rb"))
        for level in confidence_levels:
            stds = -scipy.stats.norm.ppf((1-level)/2)
            coverages.append(mean(value_in_interval_vectorized(fine_h2_range[:,newaxis], data[:,:,0], data[:,:,1], stds), 1))
            prob0.append(mean(value_in_interval_vectorized(zeros(len(fine_h2_range))[:,newaxis], data[:,:,0], data[:,:,1], stds), 1))

    export.extend(coverages)
    export.extend(prob0)

    writer = csv.writer(file(output_filename, "w"), delimiter='\t')
    writer.writerow(["h2"] + ["%s_%.2f" % (name, level) for name in ["gtex", "luric", "nfbc"] for level in confidence_levels] \
                           + ["%s_%.2f_p0" % (name, level) for name in ["gtex", "luric", "nfbc"] for level in confidence_levels])
    writer.writerows(array(export).T)

def plot_accurate_cis(output_filename=os.path.join(OUTPUT_DIR, "accurate_cis.dat")):
    export = []
    export.append(fine_h2_range)

    coverages = []
    prob0 = []    
    for filename, est_filename in zip(["gtex_h2_distribution.dat", "luric_h2_distribution.dat", "nfbc_filtered_h2_distribution.dat"],
                                      ["gtex_est_stds_1000_10000.pcl", "luric_est_stds_1000_10000.pcl", "nfbcfil_est_stds_1000_10000.pcl"]):
        dist = array(list(csv.reader(file(os.path.join(OUTPUT_DIR, filename), "r"), delimiter='\t')))[1:,1:].astype(float).T
        data = cPickle.load(file(os.path.join(OUTPUT_DIR, est_filename), "rb"))

        for level in confidence_levels:
            print filename, level
            acceptance_regions = [build_acceptance_region_given_distribution(h2_range, dist[i,:], h2, level) for i,h2 in enumerate(fine_h2_range)]
            coverage = []
            p0 = []
            for i,h2 in enumerate(fine_h2_range):
                cis = build_heritability_cis(acceptance_regions, fine_h2_range, data[i,:,0])
                coverage.append(mean((cis[:,0]<=h2) & (h2<=cis[:,1])))
                p0.append(mean(numpy.isclose(cis[:,0],  0)))
            
            coverages.append(coverage)
            prob0.append(p0)

    export.extend(coverages)
    export.extend(prob0)


    writer = csv.writer(file(output_filename, "w"), delimiter='\t')
    writer.writerow(["h2"] + ["%s_%.2f" % (name, level) for name in ["gtex", "luric", "nfbc"] for level in confidence_levels] \
                           + ["%s_%.2f_p0" % (name, level) for name in ["gtex", "luric", "nfbc"] for level in confidence_levels])
    writer.writerows(array(export).T)

def plot_khaper_accurate_cis(output_filename=os.path.join(OUTPUT_DIR, "khaper_accurate_cis.dat")):
    export = []
    export.append(fine_h2_range)

    coverages = []
    prob0 = []    
    for filename, est_filename in zip(["gtex_h2_distribution.dat", "luric_h2_distribution.dat", "nfbc_filtered_h2_distribution.dat"],
                                      ["gtex_est_stds_1000_10000.pcl", "luric_est_stds_1000_10000.pcl", "nfbcfil_est_stds_1000_10000.pcl"]):
        dist = array(list(csv.reader(file(os.path.join(OUTPUT_DIR, filename), "r"), delimiter='\t')))[1:,1:].astype(float).T
        data = cPickle.load(file(os.path.join(OUTPUT_DIR, est_filename), "rb"))

        for level in confidence_levels:
            print filename, level
            acceptance_regions = [build_khaper_acceptance_region_given_distribution(h2_range, dist[i,:], h2, level) for i,h2 in enumerate(fine_h2_range)]
            coverage = []
            p0 = []
            for i,h2 in enumerate(WT(fine_h2_range)):
                cis = build_khaper_heritability_cis(acceptance_regions, fine_h2_range, data[i,:,0], level, dist[:, 0])
                coverage.append(mean((cis[:,0]<=h2) & (h2<=cis[:,1])))
                p0.append(mean(numpy.isclose(cis[:,0],  0)))
            
            coverages.append(coverage)
            prob0.append(p0)
            
        #break

    export.extend(coverages)
    export.extend(prob0)

    #return coverages

    writer = csv.writer(file(output_filename, "w"), delimiter='\t')
    writer.writerow(["h2"] + ["%s_%.2f" % (name, level) for name in ["gtex", "luric", "nfbc"] for level in confidence_levels] \
                           + ["%s_%.2f_p0" % (name, level) for name in ["gtex", "luric", "nfbc"] for level in confidence_levels])
    writer.writerows(array(export).T)    

def plot_zero_in_cis(output_filename=os.path.join(OUTPUT_DIR, "zero_cis.dat")):
    # Thin out the h2 range so that we can use different line styles
    gcta = array(list(csv.reader(file(os.path.join(OUTPUT_DIR, "gcta_cis.dat"), "r"), delimiter='\t')))[1:,1:].astype(float)
    ours = array(list(csv.reader(file(os.path.join(OUTPUT_DIR, "accurate_cis.dat"), "r"), delimiter='\t')))[1:,1:].astype(float)
    
    h2_range_inds = concatenate([arange(0, 1000, 10), [1000]])

    export = []
    export.append(h2_range)
    export.extend(list(hstack([gcta[h2_range_inds, -4*3:], ours[h2_range_inds, -4*3:]]).T))

    writer = csv.writer(file(output_filename, "w"), delimiter='\t')
    writer.writerow(["h2"] + ["%s_%.2f_gcta" % (name, level) for name in ["gtex", "luric", "nfbc"] for level in confidence_levels] \
                           + ["%s_%.2f_accurate" % (name, level) for name in ["gtex", "luric", "nfbc"] for level in confidence_levels])
    writer.writerows(array(export).T)



def do_all():
    pass

